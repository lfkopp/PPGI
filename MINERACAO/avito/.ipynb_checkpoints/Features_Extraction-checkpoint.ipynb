{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting demand likelihood on Ativo's classified advertisements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> No ramo do comércio eletrônico, a venda de produtos através de anúncios classificados em sites conhece um crescimento rápido. No entanto, é comum que anunciantes, indivíduos ou empresas, se frustram com uma demanda fraca ou inexistente por seus produtos apesar do investimento em propaganda no caso de anunciantes empresariais. Por outro lado, uma demanda alta e persistente por produtos já vendidos causa frustração do anunciante por indicar a venda por um preço a abaixo do preço do mercado. A venda efetiva dos produtos nos sites classificados depende de uma combinação de fatores, que podem fazer uma grande diferença para aumentar o interesse dos consumidores nos produtos.  Exemplos desses fatores incluem descrições, imagens, regiões geográficas, etc. Por exemplo, descrições com nuances ou com poucos detalhes e imagens de pouca qualidade podem reduzir o interesse dos consumidores. \n",
    "\n",
    ">A ATIVO, maior site de anúncios classificados na Rússia, está acostumado com a frustração dos seus anunciantes e busca formas de reduzi-la. Com esse objetivo, ela patrocina uma competição iniciada no mês de maio de 2018 através da Kaggle.  A  Kaggle é uma empresa, que possui uma plataforma aberta na qual cientistas, pesquisadores e profissionais do mundo inteiro na área de ciência de dados são desafiados a resolver problemas reais e complexos de empresas por meio de competições. \n",
    "A “competição Ativo” da Kaggle desafia pesquisadores a predizer a probabilidade da compra de qualquer produto em função das informações fornecidas no anúncio da plataforma da ATIVO. Essas informações incluem a descrição (título, detalhes, etc.), o contexto (a cidade onde o anúncio foi emitido e anúncios já postados) e o histórico da demanda para anúncios similares no contexto. O dataset que reúne informações de todos os anúncios classificados possui cerca de 7 GB. A competição já conta com mais de 1000 equipes. Com as previsões, a ATIVO espera dar um retorno aos anunciantes sobre como otimizar suas carteiras de produtos, além de fornecer aos anunciantes indicadores sobre o potencial  interesse dos consumidores em seus anúncios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "ERROR:tornado.general:Uncaught exception in ZMQStream callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2666, in run_cell\n",
      "    self.events.trigger('post_run_cell', result)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\events.py\", line 88, in trigger\n",
      "    func(*args, **kwargs)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\ipykernel\\pylab\\backend_inline.py\", line 160, in configure_once\n",
      "    activate_matplotlib(backend)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py\", line 311, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\", line 231, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\", line 1410, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"C:\\Anaconda3\\lib\\importlib\\__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 618, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "  File \"C:\\Anaconda3\\lib\\traceback.py\", line 193, in format_stack\n",
      "    return format_list(extract_stack(f, limit=limit))\n",
      "  File \"C:\\Anaconda3\\lib\\traceback.py\", line 207, in extract_stack\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\n",
      "  File \"C:\\Anaconda3\\lib\\traceback.py\", line 358, in extract\n",
      "    f.line\n",
      "  File \"C:\\Anaconda3\\lib\\traceback.py\", line 282, in line\n",
      "    self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "  File \"C:\\Anaconda3\\lib\\linecache.py\", line 16, in getline\n",
      "    lines = getlines(filename, module_globals)\n",
      "  File \"C:\\Anaconda3\\lib\\linecache.py\", line 47, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"C:\\Anaconda3\\lib\\linecache.py\", line 136, in updatecache\n",
      "    with tokenize.open(fullname) as fp:\n",
      "  File \"C:\\Anaconda3\\lib\\tokenize.py\", line 452, in open\n",
      "    buffer = _builtin_open(filename, 'rb')\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import preprocessing, utils\n",
    "from sklearn.svm import SVC,SVR\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv.zip', compression='zip')\n",
    "print(df_train.shape)\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_ = pd.read_csv('test.csv.zip', compression='zip')\n",
    "print(df_test_.shape)\n",
    "df_test_.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_submission=pd.read_csv('sample_submission.csv')\n",
    "print(df_sample_submission.shape)\n",
    "df_sample_submission.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.concat([df_test_, df_sample_submission], axis=0)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorando os dados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exibindo 5 anúncios. É possivel ver que os dados são em russo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regios e seus numeros de anúncios\n",
    "region_ads_number= df_train['region'].value_counts()\n",
    "#Guardar os nomes das regiões\n",
    "regions_names = np.array(region_ads_number.index)\n",
    "#Guardar os números de anuncios por região.\n",
    "ads_numbers= np.array(region_ads_number)\n",
    "#Criar um dataframe com as informações anterios\n",
    "df_region_ads = pd.DataFrame({'region': regions_names, 'number of ads': ads_numbers})\n",
    "rank = np.arange(0, len(df_region_ads))\n",
    "plt.barh(rank, df_region_ads['number of ads'])\n",
    "plt.yticks(rank, df_region_ads['region'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qualidade alta\n",
    "df_train_A=df_train.query(\"deal_probability>=0.67\") \n",
    "#Qualidade Media \n",
    "df_train_M=df_train.query(\"deal_probability>=0.33 and deal_probability<0.67\") \n",
    "#Qualidade Baixa \n",
    "df_train_B=df_train.query(\"deal_probability<0.33\") \n",
    "\n",
    "print('Qde de rows Qualidade alta.........:',len(df_train_A.index))\n",
    "print('Qde de rows Qualidade Media........:',len(df_train_M.index))\n",
    "print('Qde de rows Qualidade muito baixa..:', len(df_train_B.index))\n",
    "\n",
    "number_of_ads=df_train.shape[0]\n",
    "number_of_ads==len(df_train_A.index)+len(df_train_M.index)+len(df_train_B.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test=pd.read_csv('test.csv.zip', compression='zip')\n",
    "#print('Test Size=', len(df_test.index))\n",
    "#df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_submission\n",
    "#df_sample_submission=pd.read_csv('sample_submission.csv')\n",
    "#print('Sample submission Size=', len(df_sample_submission.index))\n",
    "#df_sample_submission.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(df_sample_submission['deal_probability'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-tratamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_A=len(df_train_A.index)/number_of_ads\n",
    "perc_M=len(df_train_M.index)/number_of_ads\n",
    "perc_B=len(df_train_B.index)/number_of_ads\n",
    "print(perc_A, '-',perc_M,'-', perc_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction of 40.000 rows for trains and 20000 for tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_size=40000\n",
    "test_sample_size=20000\n",
    "A_sample_size=int(perc_A*train_sample_size)\n",
    "M_sample_size=int(perc_M*train_sample_size)\n",
    "B_sample_size=train_sample_size-(A_sample_size+M_sample_size)\n",
    "print('A size:', A_sample_size, '- M size: ', M_sample_size, '- B size : ',B_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "df_A=df_train_A.sample(n=A_sample_size, random_state=random.randint(1,A_sample_size))\n",
    "df_M=df_train_M.sample(n=M_sample_size, random_state=random.randint(1,M_sample_size))\n",
    "df_B=df_train_B.sample(n=B_sample_size, random_state=random.randint(1,B_sample_size))\n",
    "\n",
    "df_test_sample=df_test.sample(n=test_sample_size, random_state=random.randint(1,test_sample_size))\n",
    "print('A: ', df_A.shape, 'B: ', df_M.shape,' C:', df_B.shape,' test:', df_test_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCorpus(row):\n",
    "    desc=str(row['description'])\n",
    "    tit=str(row['title'])\n",
    "    #return str(desc.join(' ').join(tit))\n",
    "    return desc+' '+tit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_A_=df_A[['description']].fillna('')\n",
    "#df_M_=df_M[['description']].fillna('')\n",
    "#df_B_=df_B[['description']].fillna('')\n",
    "#df_test_sample_=df_test_sample[['description']].fillna('')\n",
    "\n",
    "df_A_=df_A.fillna('')\n",
    "df_M_=df_M.fillna('')\n",
    "df_B_=df_B.fillna('')\n",
    "df_test_sample_=df_test_sample.fillna('')\n",
    "\n",
    "\n",
    "df_A_['corpus']=df_A_.apply(getCorpus, axis=1)\n",
    "df_M_['corpus']=df_M_.apply(getCorpus, axis=1)\n",
    "df_B_['corpus']=df_B_.apply(getCorpus, axis=1)\n",
    "df_test_sample_['corpus']=df_test_sample_.apply(getCorpus, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_A_[['description','title','corpus']].head(1)\n",
    "#print('A: ', len(df['title']), 'B: ', len(df['description']),' C:', len(df['corpus']))\n",
    "df['title']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['description'].values)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['corpus'].values)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['title'].values)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remover pontuações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converter para minusculo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos nessa primeira análise considerar apenas as probalidade entre 0.33 e 0.66.\n",
    "Preenchemos com espaço as descrições sem valores dos anúncios dessas probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sample_=pd.concat([df_A_, df_M_, df_B_], axis=0)\n",
    "#df_test_sample_\n",
    "print('sample size and test size :', df_train_sample_.shape, df_test_sample_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trte_sample=pd.concat([df_train_sample_, df_test_sample_], axis=0)\n",
    "df_trte_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the corpus\n",
    "#train_corpus=list(df_train_sample_['corpus'].values)\n",
    "#test_corpus=list(df_test_sample_['corpus'].values)\n",
    "#print('sample and test lengths:', len(train_corpus), len(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the corpus\n",
    "corpus=list(df_trte_sample['corpus'].values)\n",
    "print('Corpus length:', len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove pontuaction\n",
    "#lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features extraction\n",
    "Only text on description column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\", ignore_stopwords=True)\n",
    "\n",
    "#Criamos uma subclasse de CountVectorizer, que usará o snowballstemmer\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instanciar e configuramos o vectorizer com stop words russos. Os mesmos serão desconsiderados na construção dos tokens.\n",
    "* Construimos os tokens, que são as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_vectorizer = StemmedCountVectorizer(stop_words=stopwords.words('russian'))\n",
    "\n",
    "#Extrair um array de descriptions tratadas acima\n",
    "#corpus=list(df_train_M_['description'].values)\n",
    "\n",
    "#Transformar usando a classe stemmed_vectorizer\n",
    "X_vect = stemmed_vectorizer.fit_transform(corpus)\n",
    "#X_test_vect = stemmed_vectorizer.fit_transform(test_corpus)\n",
    "X_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformar de novo para obter as features\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_tfidf = tfidf_transformer.fit_transform(X_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features\n",
    "X=X_tfidf\n",
    "#Targets\n",
    "y=df_trte_sample['deal_probability'].values \n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraimos parte das features e target para o treinamento. E outra parte para os testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=train_sample_size #n=32\n",
    "#features and targets for trains\n",
    "X_train=X[:n]\n",
    "y_train=y[:n]\n",
    "#features and targets for tests\n",
    "X_test=X[n:]\n",
    "y_test=y[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previsão usando regressão com SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=None\n",
    "clf = SVR() \n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicted :', y_hat)\n",
    "print('Expected  :', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos calcular o erro da previsão com variancia e desvio padrão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcular a variancia de qualquer probalidade estimada\n",
    "def setVariance(row):\n",
    "    \"\"\"\n",
    "    Entrada:\n",
    "        row:tupla de dataframe\n",
    "    Saída:\n",
    "        variancia:float\n",
    "    \"\"\"\n",
    "    return (row['predicted_deal_probality']-row['expected_deal_probality'])**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Criar dataframe com as demandas/probabilidades previstas e as estimadas pelo classificador\n",
    "* Aplicar a função em cada tupla para obter a variancia da probabilidade estimada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proba = pd.DataFrame({'expected_deal_probality': y_test,\n",
    "                         'predicted_deal_probality': y_hat})\n",
    "#Cria-se uma nova coluna no df\n",
    "df_proba['variance'] = df_proba.apply(setVariance, axis=1)\n",
    "df_proba.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determinando o desvio padrão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(df_proba[['variance']].sum()/len(df_proba.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos calcular a variancia e desvio padrão considerando a media da amostra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=np.mean(y_test)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setVariance2(row):\n",
    "    return (row['predicted_deal_probality']-0.43571)**2\n",
    "\n",
    "df_proba['variance'] = df_proba.apply(setVariance2, axis=1)\n",
    "df_proba.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Devio padrão\n",
    "desvio=np.sqrt(df_proba[['variance']].sum()/len(df_proba.index))\n",
    "print(\"desvio\", desvio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previsão usando Regressão Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg_predictor=None\n",
    "rg_predictor= LinearRegression()\n",
    "rg_predictor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat=rg_predictor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_hat[:5])\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previsão usando classificação com Multinomial Naive Bayes\n",
    "> Naive Bayes classifier for multinomial models\n",
    "> The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como os targets são valores continuos é necessário codifica-los para valores discretos, pois a classificação não viável para valores continuos. Senão um erro ocorre no classificador SVC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro, verificamos que o array y_test contem realmente valores contínuos, que são ideais para a regressão. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(utils.multiclass.type_of_target(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instanciando o codificador\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "#Encoded armazena valores discretos\n",
    "y_train_encoded = lab_enc.fit_transform(y_train)\n",
    "y_train_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testando que a conevrsão discretizou os targets em valores multiclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(utils.multiclass.type_of_target(y_train_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instanciar e treinar\n",
    "clf=None\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predizer\n",
    "y_hat_encoded=clf.predict(X_test)\n",
    "print(y_hat_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converter e encontrar as probabilidades\n",
    "y_hat=lab_enc.inverse_transform(y_hat_encoded)\n",
    "print(y_hat[:5])\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previsão usando classificação: SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Novamente como trata-se de uma classificação onde os targets são valores continuos é necessário codifica-los para valores discretoss, senão um erro ocorre no classificador SVC. Usamos os variaveis definida acima no caso mnb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciando e treinando o classificador. Alteramos o kernel para 'linear', pois o valor default ('rbf') nem sempre realiza bem a classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc=None\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train,y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted=svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_encoded=lab_enc.inverse_transform(y_hat_encoded)\n",
    "print(y_hat[:5])\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
