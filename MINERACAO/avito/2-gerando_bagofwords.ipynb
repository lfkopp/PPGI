{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle: Avito Demand Prediction Challenge\n",
    "source: https://www.kaggle.com/c/avito-demand-prediction\n",
    "\n",
    "\n",
    "Intro: <font color=\"red\"> criar descri√ß√£o</font>\n",
    "\n",
    "\n",
    "Team: <font color=\"red\"> criar nome</font>\n",
    "- Luis Filipe Kopp (lfkopp)\n",
    "- Lucas P√©rez (lucascperez)\n",
    "- Raphael Reis (raphacoelho)\n",
    "\n",
    "# Part two - generating bag of words \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk import download\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lfkop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "dir = 'c:/_dados/_avito/'\n",
    "download('punkt')\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "stop2 = [u'—è', u'–∞', u'–¥–∞', u'–Ω–æ', u'—Ç–µ–±–µ', u'–º–Ω–µ', u'—Ç—ã', u'–∏', u'—É', u'–Ω–∞', u'—â–∞', u'–∞–≥–∞',\n",
    "u'—Ç–∞–∫', u'—Ç–∞–º', u'–∫–∞–∫–∏–µ', u'–∫–æ—Ç–æ—Ä—ã–π', u'–∫–∞–∫–∞—è', u'—Ç—É–¥–∞', u'–¥–∞–≤–∞–π', u'–∫–æ—Ä–æ—á–µ', u'–∫–∞–∂–µ—Ç—Å—è', u'–≤–æ–æ–±—â–µ',\n",
    "u'–Ω—É', u'–Ω–µ', u'—á–µ—Ç', u'–Ω–µ–∞', u'—Å–≤–æ–∏', u'–Ω–∞—à–µ', u'—Ö–æ—Ç—è', u'—Ç–∞–∫–æ–µ', u'–Ω–∞–ø—Ä–∏–º–µ—Ä', u'–∫–∞—Ä–æ—á', u'–∫–∞–∫-—Ç–æ',\n",
    "u'–Ω–∞–º', u'—Ö–º', u'–≤—Å–µ–º', u'–Ω–µ—Ç', u'–¥–∞', u'–æ–Ω–æ', u'—Å–≤–æ–µ–º', u'–ø—Ä–æ', u'–≤—ã', u'–º', u'—Ç–¥',\n",
    "u'–≤—Å—è', u'–∫—Ç–æ-—Ç–æ', u'—á—Ç–æ-—Ç–æ', u'–≤–∞–º', u'—ç—Ç–æ', u'—ç—Ç–∞', u'—ç—Ç–∏', u'—ç—Ç–æ—Ç', u'–ø—Ä—è–º', u'–ª–∏–±–æ', u'–∫–∞–∫', u'–º—ã',\n",
    "u'–ø—Ä–æ—Å—Ç–æ', u'–±–ª–∏–Ω', u'–æ—á–µ–Ω—å', u'—Å–∞–º—ã–µ', u'—Ç–≤–æ–µ–º', u'–≤–∞—à–∞', u'–∫—Å—Ç–∞—Ç–∏', u'–≤—Ä–æ–¥–µ', u'—Ç–∏–ø–∞', u'–ø–æ–∫–∞', u'–æ–∫']\n",
    "stop = set([x.lower() for x in (stopwords.words('russian') + stop2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[ü¶ã‚≠êÔºÅÔøΩÔøΩÔøΩÔøΩÔøΩü•Ä‚§¥Ô∏èü§òü¶Ñü§∞ü§öüÜó‚¨õÔ∏è‚≠ïÔ∏èü¶â‚¨ÜÔ∏è‚¨Ü‚¨áü§∏üÜïü§£ü§îü§ìü§óü•ÖüÜòü¶Å$„Ä∞„ÄΩÔ∏èÈßÖ‰ºùÁ´∂Ëµ∞‡Æú€©€û€©‡Æú#ü§∑ü¶ä\\U00000025-\\U00000150\\U00002000-\\U00002800\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r' ', text)\n",
    "\n",
    "def convert(frase):\n",
    "    frase = remove_emoji(frase)\n",
    "    frase = frase.replace(str(np.NaN), ' ').replace('\"', ' ').replace('!', ' ').replace('#', ' ')\n",
    "    frase = re.sub(\"\\s\\s+\", \" \", frase)\n",
    "    return ' '.join([stemmer.stem(word).lower() for word in str(frase).split() if ((word not in stop) and (len(word)>2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation_date</th>\n",
       "      <th>category_name</th>\n",
       "      <th>city</th>\n",
       "      <th>deal_probability</th>\n",
       "      <th>deal_rounded</th>\n",
       "      <th>description</th>\n",
       "      <th>image_top_1</th>\n",
       "      <th>item_id</th>\n",
       "      <th>month</th>\n",
       "      <th>param_1</th>\n",
       "      <th>...</th>\n",
       "      <th>sold_0</th>\n",
       "      <th>sold_1</th>\n",
       "      <th>sold_2</th>\n",
       "      <th>sold_3</th>\n",
       "      <th>sold_4</th>\n",
       "      <th>sold_5</th>\n",
       "      <th>sold_6</th>\n",
       "      <th>sold_7</th>\n",
       "      <th>sold_8</th>\n",
       "      <th>sold_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-03-28</td>\n",
       "      <td>–¢–æ–≤–∞—Ä—ã –¥–ª—è –¥–µ—Ç–µ–π –∏ –∏–≥—Ä—É—à–∫–∏</td>\n",
       "      <td>–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥</td>\n",
       "      <td>0.12789</td>\n",
       "      <td>0.2</td>\n",
       "      <td>–ö–æ–∫–æ–Ω –¥–ª—è —Å–Ω–∞ –º–∞–ª—ã—à–∞,–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –º–µ–Ω—å—à–µ –º–µ—Å—è—Ü...</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>b912c3c6a6ad</td>\n",
       "      <td>3</td>\n",
       "      <td>–ü–æ—Å—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-03-26</td>\n",
       "      <td>–ú–µ–±–µ–ª—å –∏ –∏–Ω—Ç–µ—Ä—å–µ—Ä</td>\n",
       "      <td>–°–∞–º–∞—Ä–∞</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>–°—Ç–æ–π–∫–∞ –¥–ª—è –æ–¥–µ–∂–¥—ã, –ø–æ–¥ –≤–µ—à–∞–ª–∫–∏. –° –±—É—Ç–∏–∫–∞.</td>\n",
       "      <td>692.0</td>\n",
       "      <td>2dac0150717d</td>\n",
       "      <td>3</td>\n",
       "      <td>–î—Ä—É–≥–æ–µ</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-03-20</td>\n",
       "      <td>–ê—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ</td>\n",
       "      <td>–†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É</td>\n",
       "      <td>0.43177</td>\n",
       "      <td>0.4</td>\n",
       "      <td>–í —Ö–æ—Ä–æ—à–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏, –¥–æ–º–∞—à–Ω–∏–π –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä —Å blu ...</td>\n",
       "      <td>3032.0</td>\n",
       "      <td>ba83aefab5dc</td>\n",
       "      <td>3</td>\n",
       "      <td>–í–∏–¥–µ–æ, DVD –∏ Blu-ray –ø–ª–µ–µ—Ä—ã</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-03-25</td>\n",
       "      <td>–¢–æ–≤–∞—Ä—ã –¥–ª—è –¥–µ—Ç–µ–π –∏ –∏–≥—Ä—É—à–∫–∏</td>\n",
       "      <td>–ù–∞–±–µ—Ä–µ–∂–Ω—ã–µ –ß–µ–ª–Ω—ã</td>\n",
       "      <td>0.80323</td>\n",
       "      <td>0.8</td>\n",
       "      <td>–ü—Ä–æ–¥–∞–º –∫—Ä–µ—Å–ª–æ –æ—Ç0-25–∫–≥</td>\n",
       "      <td>796.0</td>\n",
       "      <td>02996f1dd2ea</td>\n",
       "      <td>3</td>\n",
       "      <td>–ê–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã–µ –∫—Ä–µ—Å–ª–∞</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-03-16</td>\n",
       "      <td>–ê–≤—Ç–æ–º–æ–±–∏–ª–∏</td>\n",
       "      <td>–í–æ–ª–≥–æ–≥—Ä–∞–¥</td>\n",
       "      <td>0.20797</td>\n",
       "      <td>0.2</td>\n",
       "      <td>–í—Å–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Ç–µ–ª–µ—Ñ–æ–Ω—É.</td>\n",
       "      <td>2264.0</td>\n",
       "      <td>7c90be56d2ab</td>\n",
       "      <td>3</td>\n",
       "      <td>–° –ø—Ä–æ–±–µ–≥–æ–º</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  activation_date               category_name              city  \\\n",
       "0      2017-03-28  –¢–æ–≤–∞—Ä—ã –¥–ª—è –¥–µ—Ç–µ–π –∏ –∏–≥—Ä—É—à–∫–∏      –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥   \n",
       "1      2017-03-26           –ú–µ–±–µ–ª—å –∏ –∏–Ω—Ç–µ—Ä—å–µ—Ä            –°–∞–º–∞—Ä–∞   \n",
       "2      2017-03-20               –ê—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ    –†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É   \n",
       "3      2017-03-25  –¢–æ–≤–∞—Ä—ã –¥–ª—è –¥–µ—Ç–µ–π –∏ –∏–≥—Ä—É—à–∫–∏  –ù–∞–±–µ—Ä–µ–∂–Ω—ã–µ –ß–µ–ª–Ω—ã   \n",
       "4      2017-03-16                  –ê–≤—Ç–æ–º–æ–±–∏–ª–∏         –í–æ–ª–≥–æ–≥—Ä–∞–¥   \n",
       "\n",
       "   deal_probability  deal_rounded  \\\n",
       "0           0.12789           0.2   \n",
       "1           0.00000           0.0   \n",
       "2           0.43177           0.4   \n",
       "3           0.80323           0.8   \n",
       "4           0.20797           0.2   \n",
       "\n",
       "                                         description  image_top_1  \\\n",
       "0  –ö–æ–∫–æ–Ω –¥–ª—è —Å–Ω–∞ –º–∞–ª—ã—à–∞,–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –º–µ–Ω—å—à–µ –º–µ—Å—è—Ü...       1008.0   \n",
       "1          –°—Ç–æ–π–∫–∞ –¥–ª—è –æ–¥–µ–∂–¥—ã, –ø–æ–¥ –≤–µ—à–∞–ª–∫–∏. –° –±—É—Ç–∏–∫–∞.        692.0   \n",
       "2  –í —Ö–æ—Ä–æ—à–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏, –¥–æ–º–∞—à–Ω–∏–π –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä —Å blu ...       3032.0   \n",
       "3                             –ü—Ä–æ–¥–∞–º –∫—Ä–µ—Å–ª–æ –æ—Ç0-25–∫–≥        796.0   \n",
       "4                           –í—Å–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Ç–µ–ª–µ—Ñ–æ–Ω—É.       2264.0   \n",
       "\n",
       "        item_id  month                      param_1   ...   sold_0 sold_1  \\\n",
       "0  b912c3c6a6ad      3    –ü–æ—Å—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏   ...        0      0   \n",
       "1  2dac0150717d      3                       –î—Ä—É–≥–æ–µ   ...        0      0   \n",
       "2  ba83aefab5dc      3  –í–∏–¥–µ–æ, DVD –∏ Blu-ray –ø–ª–µ–µ—Ä—ã   ...        0      0   \n",
       "3  02996f1dd2ea      3         –ê–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã–µ –∫—Ä–µ—Å–ª–∞   ...        1      1   \n",
       "4  7c90be56d2ab      3                   –° –ø—Ä–æ–±–µ–≥–æ–º   ...        0      0   \n",
       "\n",
       "  sold_2  sold_3 sold_4 sold_5 sold_6 sold_7 sold_8  sold_9  \n",
       "0      0       0      0      0      1      0      0       0  \n",
       "1      0       0      0      0      0      0      0       0  \n",
       "2      0       0      0      0      1      0      1       1  \n",
       "3      1       1      1      1      0      0      1       1  \n",
       "4      0       0      0      1      0      0      0       0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2011862, 32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_pickle(dir + 'df2.pickle')\n",
    "display(df.head())\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 –õ–∏—á–Ω—ã–µ –≤–µ—â–∏\n",
      "1 –î–ª—è –¥–æ–º–∞ –∏ –¥–∞—á–∏\n",
      "2 –ë—ã—Ç–æ–≤–∞—è —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞\n",
      "3 –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç\n",
      "4 –ù–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å\n",
      "5 –ñ–∏–≤–æ—Ç–Ω—ã–µ\n",
      "6 –•–æ–±–±–∏ –∏ –æ—Ç–¥—ã—Ö\n",
      "7 –£—Å–ª—É–≥–∏\n",
      "8 –î–ª—è –±–∏–∑–Ω–µ—Å–∞\n"
     ]
    }
   ],
   "source": [
    "groups = df.parent_category_name.unique()\n",
    "for i,group in enumerate(groups):\n",
    "    print(i,group)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[['item_id', 'parent_category_name', 'source', 'deal_probability', 'title', 'description']]\n",
    "for i,group in enumerate(groups):\n",
    "    df3 = df2[df2['parent_category_name'] == group]\n",
    "    #df3 = df3[df3['source'] == 'train']\n",
    "    with open(dir+'consolidated_'+str(i)+'.txt', 'wb') as f: \n",
    "        with open(dir + 'stemmed_'+str(i)+'.txt', 'wb') as s: \n",
    "            for line, row in df3.iterrows():\n",
    "                content = ''\n",
    "                idx = row.item_id\n",
    "                desc = convert(str(row.title) + ' ' + str(row.description)).split()\n",
    "                deal = str(np.round(row.deal_probability,5))\n",
    "                src = str(row.source)\n",
    "                con = Counter(desc).items()\n",
    "                st = str(group) + ';' +str(idx) + ';' +str(src) + ';' +str(deal) + ';'\n",
    "                st2 = []\n",
    "                for item in con:\n",
    "                    a,b = item\n",
    "                    content += str(group) + ';' +str(idx) + ';' +str(src) + ';' +str(deal) + ';' + str(a) + ';' + str(b) + \"\\r\\n\"\n",
    "                    st2.append(a)\n",
    "                st += ' '.join(list(set(st2)))\n",
    "                st += \"\\r\\n\"\n",
    "                f.write(content.encode('utf-8'))\n",
    "                s.write(st.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i,group in enumerate(groups):\n",
    "    df5 = pd.read_csv(dir + 'consolidated_'+str(i)+'.txt', sep=';', names=['parent_category', 'item_id', 'source', 'deal_probability', 'word_stem', 'number'])\n",
    "    df6 = pd.read_csv(dir + 'stemmed_'+str(i)+'.txt', sep=';', names=['parent_category', 'item_id', 'source', 'deal_probability', 'phrase_stem'])\n",
    "    df5['greater'] = df5.deal_probability > 0.3\n",
    "    df5['less'] = df5.deal_probability < 0.3\n",
    "    dw = pd.DataFrame(df5[df5.source == 'train'].groupby('word_stem')[['number','deal_probability','greater','less']].sum())\n",
    "    dw['perc_greater'] = dw.greater / (dw.greater + dw.less)\n",
    "    dw.sort_values(by='number', ascending=False, inplace=True)\n",
    "    dw = dw[:8000]\n",
    "    dw['avg_deal'] = np.round(dw.deal_probability / dw.number ,4)\n",
    "    mean = dw['avg_deal'].mean()\n",
    "    std = dw['avg_deal'].std()\n",
    "    dw['avg_deal_normal'] = (dw['avg_deal']-mean)/std\n",
    "    dw['avg_deal_normal_sq'] = dw['avg_deal_normal'] ** 2\n",
    "    dw.sort_values(by='avg_deal_normal_sq', ascending=False, inplace=True)\n",
    "    dw = dw[:4000]\n",
    "    dw.to_pickle(dir + 'dw_' + str(i) + '.pickle')\n",
    "    words = dw.index.tolist()\n",
    "    with open(dir+'matrix_'+str(i)+'.txt', 'wb') as f:\n",
    "        f.write(('item_id;source;deal_probability;'+';'.join(words)).encode('utf-8') + \"\\r\\n\")\n",
    "        for j,l in df6.iterrows():\n",
    "            linha = str(l.item_id) + ';' + str(l.source) + ';' + str(np.round(l.deal_probability,5)) \n",
    "            v = str(l.phrase_stem).split()\n",
    "            for word in words:\n",
    "                linha += ';'+ str((word.lower() in v)*1)\n",
    "            linha += \"\\r\\n\"\n",
    "            f.write(linha.encode('utf-8'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.read_csv(dir + 'matrix_5.txt', sep=';').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d['used'] = '–±\\—É'\n",
    "#d['forfree'] = '–±–µ—Å–ø–ª–∞—Ç–Ω–æ'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
