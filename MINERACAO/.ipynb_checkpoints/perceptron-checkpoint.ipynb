{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HmJ02x6qWTr0"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PNhQZ0piWTr5"
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.1):\n",
    "        self._input_size = 0\n",
    "        self._output_size = 0\n",
    "        self._weights = None\n",
    "        self.b = None\n",
    "        self._num_samples = 0\n",
    "        self._lr = learning_rate\n",
    "\n",
    "    \n",
    "    def fit(self, X, y, num_epochs = 100, batch_size=None, verbose=False):\n",
    "        if type(X) is not np.ndarray:\n",
    "            raise Exception('X deve ser uma matrix do tipo numpy.ndarray')\n",
    "\n",
    "        if type(y) is not np.ndarray:\n",
    "            raise Exception('y deve ser um vetor do tipo numpy.ndarray')\n",
    "    \n",
    "        \n",
    "        # hot encode y caso ele não venha neste formato\n",
    "        if len(y.shape) == 1: # se só for um vetor então o shape só vai retornar um valor\n",
    "            old_y = y.copy()\n",
    "            num_class = len(set(old_y))\n",
    "            \n",
    "            # transforma cada y em um vetor do tamanho do número de classes\n",
    "            y = np.zeros((old_y.shape[0], 3))\n",
    "\n",
    "            # seta para 1 as posições dos perceptrons correspondentes as classes\n",
    "            for i in xrange(old_y.shape[0]):\n",
    "                posi = old_y[i]\n",
    "                y[i][posi] = 1\n",
    "        \n",
    "      \n",
    "        # batch size é o número de elementos que serão usados em cada época pra atualizar o peso\n",
    "        # caso não seja definido, vou utilizar todos os dados de treinamento disponíveis\n",
    "        if batch_size is None:\n",
    "            batch_size = X.shape[0]\n",
    "        \n",
    "        \n",
    "        # armazenando parâmetros dos dados\n",
    "        self._num_samples = X.shape[0]\n",
    "        self._input_size  = X.shape[1]\n",
    "        self._output_size = y.shape[1]\n",
    "        \n",
    "        # cada neurônio vai ter um conjunto de pesos igual ao número de entradas, \n",
    "        self._weights =  np.random.rand(self._output_size, self._input_size)\n",
    "        \n",
    "        # cada neurônio tem seu próprio bias\n",
    "        self._b =  np.ones(self._output_size)\n",
    "        \n",
    "        \n",
    "        # aqui inicializa o treinamento\n",
    "        for i in range(num_epochs):\n",
    "\n",
    "            # pego um conjunto de posições representando elemento da amostra igual ao tamanho do batch size\n",
    "            posi_to_sample = np.random.randint(0, self._num_samples, batch_size)\n",
    "            for posi in posi_to_sample:\n",
    "                \n",
    "                # obtendo um elemento específico                 \n",
    "                x = X[posi]\n",
    "                y_expected = y[posi]\n",
    "\n",
    "                # calculando Z\n",
    "                Z = np.dot(self._weights, x) + self._b\n",
    "\n",
    "                #  calculando a saída dos neurônios\n",
    "                y_hat = self._activation_function(Z)\n",
    "\n",
    "                # calculando parte do gradiente            \n",
    "                error = (y_hat  - y_expected)\n",
    "                activation_derivative = self._activation_function_derivative(Z)\n",
    "                delta =  error * activation_derivative # delta é só parte da derivada, vou gerar o gradiente completo abaixo\n",
    "                                \n",
    "                #  calculando o gradiente para cada neurônio e atualizando seus pesos\n",
    "                for k in range(self._output_size):        \n",
    "                    \n",
    "                    gradient_w = (1.0/batch_size) * np.dot(delta[k] , x) # como troquei a minha função de erro pra erro médio, o gradiente fica com 1/n\n",
    "                    gradient_b = (1.0/batch_size) * delta[k]\n",
    "\n",
    "                    # finalmente atualizando os pesos\n",
    "                    self._weights[k] = self._weights[k] - (self._lr * gradient_w)\n",
    "                    self._b[k] = self._b[k] - (self._lr * gradient_b)\n",
    "            \n",
    "            \n",
    "            # pra não imprimir sempre, vou mostrar acc a cada 10 iterações\n",
    "            if verbose and (i + 1) % 10 == 0:\n",
    "                print (\"Epoch[%d]: acc: %f \" %(i+1,  self._acc(X, y) ))\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Z é um vetor, cada neurônio vai ter uma saida, portando, o shape de Z é igual a (num_exemplos, num_classes)\n",
    "        Z = np.array([ np.dot(self._weights, x) + self._b  for x in X])\n",
    "        \n",
    "        # aplicando a função de ativação\n",
    "        output = self._activation_function(Z)\n",
    "        \n",
    "        # como o y gerado é um vetor, estou retornando somente as classes relacionadas\n",
    "        return np.argmax(output, axis=1)\n",
    "        \n",
    "    \n",
    "    def _activation_function(self, Z):\n",
    "        return 1.0 / (1.0 + np.e**(-Z))\n",
    "    \n",
    "    def _activation_function_derivative(self, Z):\n",
    "        res_activation = self._activation_function(Z)\n",
    "        return res_activation * (1 - res_activation) \n",
    "    \n",
    "    \n",
    "    def _acc(self, X, y):\n",
    "        count = 0\n",
    "        y_hat = self.predict(X)\n",
    "        for i in range(len(y)):\n",
    "            if np.argmax(y[i]) == y_hat[i]:\n",
    "                count += 1\n",
    "\n",
    "        return float(count) / len(y)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2584
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1602,
     "status": "ok",
     "timestamp": 1525390508743,
     "user": {
      "displayName": "Victor Garritano",
      "photoUrl": "//lh3.googleusercontent.com/-hA5KLMXLdZA/AAAAAAAAAAI/AAAAAAAADZA/wNQbKvZo3KI/s50-c-k-no/photo.jpg",
      "userId": "115570807724700621686"
     },
     "user_tz": 180
    },
    "id": "1BgAruyaWTr-",
    "outputId": "02803197-29f0-4880-9be6-d0dc8e939356"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[10]: acc: 0.352381 \n",
      "Epoch[20]: acc: 0.352381 \n",
      "Epoch[30]: acc: 0.352381 \n",
      "Epoch[40]: acc: 0.352381 \n",
      "Epoch[50]: acc: 0.352381 \n",
      "Epoch[60]: acc: 0.352381 \n",
      "Epoch[70]: acc: 0.352381 \n",
      "Epoch[80]: acc: 0.352381 \n",
      "Epoch[90]: acc: 0.352381 \n",
      "Epoch[100]: acc: 0.352381 \n",
      "Epoch[110]: acc: 0.352381 \n",
      "Epoch[120]: acc: 0.352381 \n",
      "Epoch[130]: acc: 0.352381 \n",
      "Epoch[140]: acc: 0.352381 \n",
      "Epoch[150]: acc: 0.352381 \n",
      "Epoch[160]: acc: 0.352381 \n",
      "Epoch[170]: acc: 0.361905 \n",
      "Epoch[180]: acc: 0.361905 \n",
      "Epoch[190]: acc: 0.380952 \n",
      "Epoch[200]: acc: 0.409524 \n",
      "Epoch[210]: acc: 0.447619 \n",
      "Epoch[220]: acc: 0.495238 \n",
      "Epoch[230]: acc: 0.542857 \n",
      "Epoch[240]: acc: 0.571429 \n",
      "Epoch[250]: acc: 0.628571 \n",
      "Epoch[260]: acc: 0.638095 \n",
      "Epoch[270]: acc: 0.647619 \n",
      "Epoch[280]: acc: 0.647619 \n",
      "Epoch[290]: acc: 0.647619 \n",
      "Epoch[300]: acc: 0.647619 \n",
      "Epoch[310]: acc: 0.647619 \n",
      "Epoch[320]: acc: 0.647619 \n",
      "Epoch[330]: acc: 0.600000 \n",
      "Epoch[340]: acc: 0.314286 \n",
      "Epoch[350]: acc: 0.295238 \n",
      "Epoch[360]: acc: 0.295238 \n",
      "Epoch[370]: acc: 0.295238 \n",
      "Epoch[380]: acc: 0.295238 \n",
      "Epoch[390]: acc: 0.295238 \n",
      "Epoch[400]: acc: 0.295238 \n",
      "Epoch[410]: acc: 0.295238 \n",
      "Epoch[420]: acc: 0.295238 \n",
      "Epoch[430]: acc: 0.295238 \n",
      "Epoch[440]: acc: 0.295238 \n",
      "Epoch[450]: acc: 0.295238 \n",
      "Epoch[460]: acc: 0.295238 \n",
      "Epoch[470]: acc: 0.295238 \n",
      "Epoch[480]: acc: 0.295238 \n",
      "Epoch[490]: acc: 0.295238 \n",
      "Epoch[500]: acc: 0.723810 \n",
      "Epoch[510]: acc: 0.914286 \n",
      "Epoch[520]: acc: 0.914286 \n",
      "Epoch[530]: acc: 0.790476 \n",
      "Epoch[540]: acc: 0.942857 \n",
      "Epoch[550]: acc: 0.866667 \n",
      "Epoch[560]: acc: 0.895238 \n",
      "Epoch[570]: acc: 0.961905 \n",
      "Epoch[580]: acc: 0.904762 \n",
      "Epoch[590]: acc: 0.866667 \n",
      "Epoch[600]: acc: 0.742857 \n",
      "Epoch[610]: acc: 0.914286 \n",
      "Epoch[620]: acc: 0.714286 \n",
      "Epoch[630]: acc: 0.942857 \n",
      "Epoch[640]: acc: 0.904762 \n",
      "Epoch[650]: acc: 0.876190 \n",
      "Epoch[660]: acc: 0.914286 \n",
      "Epoch[670]: acc: 0.952381 \n",
      "Epoch[680]: acc: 0.914286 \n",
      "Epoch[690]: acc: 0.942857 \n",
      "Epoch[700]: acc: 0.885714 \n",
      "Epoch[710]: acc: 0.876190 \n",
      "Epoch[720]: acc: 0.885714 \n",
      "Epoch[730]: acc: 0.942857 \n",
      "Epoch[740]: acc: 0.961905 \n",
      "Epoch[750]: acc: 0.914286 \n",
      "Epoch[760]: acc: 0.904762 \n",
      "Epoch[770]: acc: 0.819048 \n",
      "Epoch[780]: acc: 0.961905 \n",
      "Epoch[790]: acc: 0.904762 \n",
      "Epoch[800]: acc: 0.809524 \n",
      "Epoch[810]: acc: 0.961905 \n",
      "Epoch[820]: acc: 0.942857 \n",
      "Epoch[830]: acc: 0.904762 \n",
      "Epoch[840]: acc: 0.952381 \n",
      "Epoch[850]: acc: 0.961905 \n",
      "Epoch[860]: acc: 0.961905 \n",
      "Epoch[870]: acc: 0.952381 \n",
      "Epoch[880]: acc: 0.923810 \n",
      "Epoch[890]: acc: 0.914286 \n",
      "Epoch[900]: acc: 0.961905 \n",
      "Epoch[910]: acc: 0.914286 \n",
      "Epoch[920]: acc: 0.961905 \n",
      "Epoch[930]: acc: 0.942857 \n",
      "Epoch[940]: acc: 0.914286 \n",
      "Epoch[950]: acc: 0.933333 \n",
      "Epoch[960]: acc: 0.923810 \n",
      "Epoch[970]: acc: 0.914286 \n",
      "Epoch[980]: acc: 0.923810 \n",
      "Epoch[990]: acc: 0.904762 \n",
      "Epoch[1000]: acc: 0.952381 \n",
      "\n",
      "\n",
      "\n",
      "##############################\n",
      "('ACC-TEST: ', 1.0)\n",
      "##############################\n",
      "y_esperado: 1 -- y_obtido: 1 \n",
      "y_esperado: 0 -- y_obtido: 0 \n",
      "y_esperado: 2 -- y_obtido: 2 \n",
      "y_esperado: 1 -- y_obtido: 1 \n",
      "y_esperado: 1 -- y_obtido: 1 \n",
      "y_esperado: 0 -- y_obtido: 0 \n",
      "y_esperado: 1 -- y_obtido: 1 \n",
      "y_esperado: 2 -- y_obtido: 2 \n",
      "y_esperado: 1 -- y_obtido: 1 \n",
      "y_esperado: 1 -- y_obtido: 1 \n",
      "y_esperado: 2 -- y_obtido: 2 \n",
      "y_esperado: 0 -- y_obtido: 0 \n",
      "y_esperado: 0 -- y_obtido: 0 \n",
      "y_esperado: 0 -- y_obtido: 0 \n",
      "y_esperado: 0 -- y_obtido: 0 \n",
      "y_esperado: 1 -- y_obtido: 1 \n",
      "y_esperado: 2 -- y_obtido: 2 \n",
      "y_esperado: 1 -- y_obtido: 1 \n",
      "y_esperado: 1 -- y_obtido: 1 \n",
      "y_esperado: 2 -- y_obtido: 2 \n",
      "y_esperado: 0 -- y_obtido: 0 \n",
      "y_esperado: 2 -- y_obtido: 2 \n",
      "y_esperado: 0 -- y_obtido: 0 \n",
      "y_esperado: 2 -- y_obtido: 2 \n",
      "y_esperado: 2 -- y_obtido: 2 \n",
      "y_esperado: 2 -- y_obtido: 2 \n",
      "y_esperado: 2 -- y_obtido: 2 \n",
      "y_esperado: 2 -- y_obtido: 2 \n",
      "y_esperado: 0 -- y_obtido: 0 \n",
      "y_esperado: 0 -- y_obtido: 0 \n",
      "y_esperado: 0 -- y_obtido: 0 \n",
      "y_esperado: 0 -- y_obtido: 0 \n",
      "y_esperado: 1 -- y_obtido: 1 \n",
      "y_esperado: 0 -- y_obtido: 0 \n",
      "y_esperado: 0 -- y_obtido: 0 \n",
      "y_esperado: 2 -- y_obtido: 2 \n",
      "y_esperado: 1 -- y_obtido: 1 \n",
      "y_esperado: 0 -- y_obtido: 0 \n",
      "y_esperado: 0 -- y_obtido: 0 \n",
      "y_esperado: 0 -- y_obtido: 0 \n",
      "y_esperado: 2 -- y_obtido: 2 \n",
      "y_esperado: 1 -- y_obtido: 1 \n",
      "y_esperado: 1 -- y_obtido: 1 \n",
      "y_esperado: 0 -- y_obtido: 0 \n",
      "y_esperado: 0 -- y_obtido: 0 \n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# X = StandardScaler().fit_transform(X) # descomente esta linha pra melhorar o resultado, acredito que fique em média mais estável\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "percep = Perceptron(learning_rate=0.2)\n",
    "percep.fit(X_train, y_train, num_epochs=1000, batch_size=32, verbose=True)\n",
    "\n",
    "# avaliando com dados não vistos\n",
    "print (\"\\n\\n\")\n",
    "print (\"#\" * 30)\n",
    "y_hat = percep.predict(X_test)\n",
    "print (\"ACC-TEST: \", accuracy_score(y_hat, y_test))\n",
    "print (\"#\" * 30)\n",
    "for i in range(len(y_hat)):\n",
    "    print (\"y_esperado: %d -- y_obtido: %d \" %(y_test[i], y_hat[i]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "i0XfPMMAWzbw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "perceptron.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
